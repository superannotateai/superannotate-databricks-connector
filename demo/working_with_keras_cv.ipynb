{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import resource\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "import keras_cv\n",
    "\n",
    "from keras_cv import bounding_box, visualizations\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "\n",
    "from petastorm.spark import SparkDatasetConverter, make_spark_converter\n",
    "from petastorm.transform import TransformSpec\n",
    "\n",
    "\n",
    "MAX_BOXES = 20\n",
    "IMG_SHAPE = (256,256)\n",
    "FOLDER_PATH = \"./images/\"\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "\n",
    "\n",
    "# Spark Session\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
    "spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, \"file:///src/object_detection/peta/\")\n",
    "\n",
    "# Load the data\n",
    "df = spark.read.parquet(\"./annotations.parquet\")[\"image_name\",\"bounding_boxes\"]\n",
    "\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "\n",
    "\n",
    "\n",
    "# Define a Pandas UDF to prepend the image path\n",
    "@pandas_udf(\"string\")\n",
    "def prepend_path(img_name):\n",
    "    return img_name.apply(lambda x: os.path.join(FOLDER_PATH, x))\n",
    "\n",
    "# Add the full path to the image names\n",
    "df = df.withColumn('image_name', prepend_path(df['image_name']))\n",
    "\n",
    "# Prepare the data for splitting\n",
    "df = df.limit(10).repartition(4)  # Use 4 partitions as an example, adjust as needed\n",
    "\n",
    "# Split the data into train and validation datasets\n",
    "df_train, df_val = df.randomSplit([0.9, 0.1], seed=12345)\n",
    "\n",
    "# Convert to Petastorm datasets\n",
    "converter_train = make_spark_converter(df_train)\n",
    "converter_val = make_spark_converter(df_val)\n",
    "\n",
    "# Your transform_row function and the rest of your code remains largely unchanged...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Your data loading code and the rest remains unchanged..\n",
    "\n",
    "\n",
    "def transform_row(pd_batch):\n",
    "    num_rows = len(pd_batch)\n",
    "\n",
    "    # Initialize arrays\n",
    "    imgs = []\n",
    "    labels = []\n",
    "\n",
    "    for idx in range(num_rows):\n",
    "        \n",
    "        # Load the image\n",
    "        # However, add error handling to the image processing step in your transform function:\n",
    "        try:\n",
    "            img = Image.open(pd_batch[\"image_name\"].iloc[idx])\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening image {pd_batch['image_name'].iloc[idx]}: {e}\")\n",
    "            continue  # Skip this image\n",
    "        \n",
    "        # Original image size\n",
    "        orig_width, orig_height = img.size\n",
    "        \n",
    "        # Resize and normalize the image\n",
    "        img = img.resize(IMG_SHAPE[::-1]).convert('RGB')\n",
    "        img = np.array(img) / 255.0\n",
    "\n",
    "        # Compute the scale factors\n",
    "        width_scale = IMG_SHAPE[1] / orig_width\n",
    "        height_scale = IMG_SHAPE[0] / orig_height\n",
    "\n",
    "        # Scale the bounding boxes\n",
    "        bboxes = np.array(pd_batch[\"bounding_boxes\"].iloc[idx])\n",
    "        bboxes[0::5] = bboxes[0::5]*width_scale # x1-coordinates\n",
    "        bboxes[2::5] = bboxes[2::5]*width_scale # x2-coordinates\n",
    "        bboxes[1::5] = bboxes[1::5]*height_scale # y1-coordinates\n",
    "        bboxes[3::5] = bboxes[3::5]*height_scale # y2-coordinates \n",
    "\n",
    "        # Pad array to MAX_BOXES*5\n",
    "        if len(bboxes) < MAX_BOXES*5:\n",
    "            bboxes = np.pad(bboxes,(0,MAX_BOXES*5-len(bboxes)),constant_values=-1)\n",
    "        else:\n",
    "            bboxes = bboxes[:MAX_BOXES*5]\n",
    "        \n",
    "        imgs.append(img)\n",
    "        labels.append(bboxes)\n",
    "\n",
    "    result_df = pd.DataFrame({\"image\": imgs, \"labels\": labels})\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# And in your TransformSpec:\n",
    "transform_spec_fn = TransformSpec(\n",
    "    transform_row,\n",
    "    edit_fields=[\n",
    "        (\"image\",np.float32,IMG_SHAPE + (3,),False),\n",
    "        (\"labels\",np.float32,(MAX_BOXES*5,),False)],\n",
    "    removed_fields=[\"image_name\",\"bounding_boxes\"]\n",
    ")\n",
    "\n",
    "\n",
    "def make_ragged_boxes_and_classes(labels):\n",
    "    t = tf.reshape(labels,(BATCH_SIZE,MAX_BOXES,5))\n",
    "    d = tf.RaggedTensor.from_tensor(t, padding=[-1,-1,-1,-1,-1])\n",
    "    classes = d[:,:,4]\n",
    "    boxes = d[:,:,:4]\n",
    "\n",
    "    return {\"boxes\":boxes,\"classes\":classes}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ragged_boxes_and_classes(labels):\n",
    "    t = tf.reshape(labels,(BATCH_SIZE,MAX_BOXES,5))\n",
    "    d = tf.RaggedTensor.from_tensor(t, padding=[-1,-1,-1,-1,-1])\n",
    "    classes = d[:,:,4]\n",
    "    boxes = d[:,:,:4]\n",
    "\n",
    "    # Return a dictionary\n",
    "    return {\"boxes\":boxes,\"classes\":classes}\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "data = None\n",
    "i,b=[],[]\n",
    "with converter_train.make_tf_dataset(transform_spec=transform_spec_fn, batch_size=BATCH_SIZE) as ds:\n",
    "    \n",
    "    ds = ds.map(lambda x: (x.image, make_ragged_boxes_and_classes(x.labels)))\n",
    "    ds = ds.map(lambda x, y: {\"image\": x, \"bounding_boxes\": y})\n",
    "    # Dataset matches KERAS specifications with bounding box format XYXY\n",
    "\n",
    "    for data in ds.take(1):\n",
    "        i.append(data)\n",
    "\n",
    "\n",
    "class_mapping = ['SYH',\n",
    "'SVB',\n",
    "'CSV',\n",
    "'CFR',\n",
    "'CDY']\n",
    "\n",
    "\n",
    "def visualize_dataset(inputs, value_range, rows, cols, bounding_box_format):\n",
    "    images, bounding_boxes = inputs[\"image\"], inputs[\"bounding_boxes\"]\n",
    "    visualization.plot_bounding_box_gallery(\n",
    "        inputs[\"image\"],\n",
    "        value_range=value_range,\n",
    "        rows=rows,\n",
    "        cols=cols,\n",
    "        y_true=bounding_boxes,\n",
    "        scale=5,\n",
    "        font_scale=0.7,\n",
    "        bounding_box_format=bounding_box_format,\n",
    "        class_mapping=class_mapping,\n",
    "    )\n",
    "\n",
    "\n",
    "class_mapping = ['SYH',\n",
    "'SVB',\n",
    "'CSV',\n",
    "'CFR',\n",
    "'CDY']\n",
    "\n",
    "model = keras_cv.models.RetinaNet.from_preset(\n",
    "    \"resnet50_imagenet\",\n",
    "    num_classes=len(class_mapping),\n",
    "    # For more info on supported bounding box formats, visit\n",
    "    # https://keras.io/api/keras_cv/bounding_box/\n",
    "    bounding_box_format=\"xyxy\",\n",
    ")\n",
    "\n",
    "\n",
    "base_lr = 0.005\n",
    "# including a global_clipnorm is extremely important in object detection tasks\n",
    "optimizer = tf.keras.optimizers.SGD(\n",
    "    learning_rate=base_lr, momentum=0.9, global_clipnorm=10.0\n",
    "    )\n",
    "\n",
    "model.compile(\n",
    "    classification_loss=\"focal\",\n",
    "    box_loss=\"smoothl1\",\n",
    "    optimizer=optimizer,\n",
    "    # We will use our custom callback to evaluate COCO metrics\n",
    "    metrics=None,\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "data = None\n",
    "\n",
    "class_mapping = ['SYH',\n",
    "'SVB',\n",
    "'CSV',\n",
    "'CFR',\n",
    "'CDY']\n",
    "\n",
    "\n",
    "class EvaluateCOCOMetricsCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.metrics = keras_cv.metrics.BoxCOCOMetrics(\n",
    "            bounding_box_format=\"xyxy\",\n",
    "            # passing 1e9 ensures we never evaluate until\n",
    "            # `metrics.result(force=True)` is\n",
    "            # called.\n",
    "            evaluate_freq=1e9,\n",
    "        )\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        self.metrics.reset_state()\n",
    "        for batch in tqdm(self.data):\n",
    "            images, y_true = batch[0], batch[1]\n",
    "            y_pred = self.model.predict(images, verbose=0)\n",
    "            self.metrics.update_state(y_true, y_pred)\n",
    "\n",
    "        metrics = self.metrics.result(force=True)\n",
    "        logs.update(metrics)\n",
    "        return logs\n",
    "    \n",
    "\n",
    "def dict_to_tuple(inputs):\n",
    "    return inputs[\"images\"], bounding_box.to_dense(\n",
    "        inputs[\"bounding_boxes\"], max_boxes=20\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with converter_train.make_tf_dataset(transform_spec=transform_spec_fn, batch_size=BATCH_SIZE) as dst, \\\n",
    "    converter_val.make_tf_dataset(transform_spec=transform_spec_fn, batch_size=BATCH_SIZE) as dsv :\n",
    "    \n",
    "    dst = dst.map(lambda x: (x.image, make_ragged_boxes_and_classes(x.labels)))\n",
    "    dst = dst.map(lambda x, y: {\"images\": x, \"bounding_boxes\": y})\n",
    "    dsv = dsv.map(lambda x: (x.image, make_ragged_boxes_and_classes(x.labels)))\n",
    "    dsv = dsv.map(lambda x, y: {\"images\": x, \"bounding_boxes\": y})\n",
    "    dsv = dsv.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dst = dst.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    model.fit(\n",
    "        dst.take(4),\n",
    "        validation_data=dsv.take(4),\n",
    "        # Run for 10-35~ epochs to achieve good scores.\n",
    "        epochs=1,\n",
    "        callbacks=[EvaluateCOCOMetricsCallback(dsv.take(4))],\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
